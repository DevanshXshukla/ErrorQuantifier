{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9amitn9SEjg"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from collections import Counter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#download destination and requirement reports from rp-dev mail chain\n",
        "#read destination report\n",
        "\n",
        "dest_rep = pd.read_csv('destination_path')     #enter correct path\n",
        "\n",
        "#convert requirement report extension from .tsv to .csv on your local device\n",
        "\n",
        "req_rep = pd.read_csv('requirement_path', encoding=\"utf-8\")     #enter correct path"
      ],
      "metadata": {
        "id": "CZrBSXjnSMZX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print head of both files to check correct reading process\n",
        "\n",
        "dest_rep.head()"
      ],
      "metadata": {
        "id": "W5KyTlzbSOD0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "req_rep.head()"
      ],
      "metadata": {
        "id": "IJg5uCOQTYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 1 : IDENTIFYING ERRORS FROM REQUIREMENT REPORT AND QUANTIFYING THEM\n"
      ],
      "metadata": {
        "id": "jX-reED66FzK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#keep only errored stn creations in req report\n",
        "\n",
        "req_rep = req_rep[req_rep['state'] == 'ERRORED']"
      ],
      "metadata": {
        "id": "n6e1RR52Ta-C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#extract error code and error message from Errors json in requirement report\n",
        "\n",
        "def extract_error_details_fixed_v3(error_str):\n",
        "    try:\n",
        "        if not isinstance(error_str, str) or error_str.strip() == \"\":\n",
        "            return \"\", \"\"\n",
        "        cleaned_str = re.sub(r\"^\\['|'\\]$\", \"\", error_str.strip())\n",
        "        cleaned_str = cleaned_str.replace(\"'\", \"\\\"\")\n",
        "        error_list = json.loads(cleaned_str)\n",
        "\n",
        "        if isinstance(error_list, list) and len(error_list) > 0:\n",
        "            error_code = error_list[0].get(\"code\", \"\").strip()\n",
        "            error_message = error_list[0].get(\"message\", \"\").strip()\n",
        "            return error_code, error_message\n",
        "\n",
        "    except json.JSONDecodeError:\n",
        "        return \"JSON_ERROR\", error_str\n",
        "    except Exception as e:\n",
        "        return \"PARSE_ERROR\", str(e)\n",
        "\n",
        "req_rep[\"error_code\"], req_rep[\"error_message\"] = zip(*req_rep[\"errors\"].astype(str).map(extract_error_details_fixed_v3))\n",
        "\n",
        "\n",
        "output_path = \"req_rep_errored.csv\"\n",
        "req_rep.to_csv(output_path, index=False, encoding=\"utf-8\")\n",
        "\n",
        "print(req_rep[[\"errors\", \"error_code\", \"error_message\"]].head())\n",
        "\n"
      ],
      "metadata": {
        "id": "WalElRN8UYtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count unique_errors and their frequency\n",
        "\n",
        "error_counts = req_rep[\"error_code\"].value_counts().reset_index()\n",
        "error_counts.columns = [\"error_name\", \"error_count\"]\n",
        "\n",
        "req_rep[\"error_name\"] = req_rep[\"error_code\"]\n",
        "req_rep[\"error_count\"] = req_rep[\"error_name\"].map(dict(zip(error_counts[\"error_name\"], error_counts[\"error_count\"])))\n",
        "\n",
        "output_path = \"req_rep_error_quantified.csv\"\n",
        "req_rep.to_csv(output_path, index=False)"
      ],
      "metadata": {
        "id": "_4hKc5CAbwLd"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2 : CHECKING QUANTITY CREATION GAPS IN MASTER POLICY AND DESTINATION REPORT\n"
      ],
      "metadata": {
        "id": "gFiRdHTC6OuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read master policy\n",
        "\n",
        "mp = pd.read.csv('master_policy_path')\n"
      ],
      "metadata": {
        "id": "GY_8UJ1feaeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keeping only relevant columns from master policy\n",
        "\n",
        "mp = mp[['fsn', 'use_case', 'cluster', 'inventory_target']]"
      ],
      "metadata": {
        "id": "Cxtu3Jt7yd_l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep only those rows in master policy where inventory target > 0\n",
        "\n",
        "mp = mp[mp['inventory_target'] > 0]"
      ],
      "metadata": {
        "id": "_1m4T6iRgIiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert cluster column entries from lower to upper case for easy merging\n",
        "\n",
        "mp['cluster'] = mp['cluster'].str.upper()"
      ],
      "metadata": {
        "id": "VKTLk5mBgXvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make unique key for merging with destination report\n",
        "\n",
        "mp['unique_key'] = mp['fsn'].astype(str) + '_' + mp['cluster'].astype(str)"
      ],
      "metadata": {
        "id": "Osi4U7orgiKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose use_case according to phase\n",
        "\n",
        "#phase 2 : hl_nl_ds\n",
        "#phase 3 : nl_ds\n",
        "#phase 4 hl_nl_ds_02\n",
        "#phase 5 : nl_ds2\n",
        "#phase 6 : hl_ds_adhoc\n",
        "\n",
        "#for example , taken nl_ds2 for phase 5\n",
        "mp = mp[mp['use_case'].isin(['nl_ds2'])]\n"
      ],
      "metadata": {
        "id": "jAyuMxp7gr6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#make unique key in destination report\n",
        "\n",
        "dest_rep['unique_key'] = dest_rep['fsn'].astype(str) + '_' + dest_rep['cluster'].astype(str)\n",
        "\n",
        "#convert unique key to upper case\n",
        "\n",
        "dest_rep['unique_key'] = dest_rep['unique_key'].str.upper()\n",
        "\n",
        "#rename a column to clearly analyze after successfull merge\n",
        "\n",
        "dest_rep.rename(columns={\"inventorytarget\": \"inventory_target_dr\"}, inplace=True)"
      ],
      "metadata": {
        "id": "Myp5Qwf8haNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#in dest_rep count the number of rows which have quantity NaN and also extract those rows and create a new file called dest _rep_qty_nan csv\n",
        "\n",
        "nan_quantity_count = dest_rep['quantity'].isna().sum()\n",
        "print(f\"Number of rows with NaN in quantity: {nan_quantity_count}\")\n",
        "\n",
        "# Extract rows with NaN in 'quantity' and save to a new file\n",
        "\n",
        "dest_rep_qty_nan = dest_rep[dest_rep['quantity'].isna()]\n",
        "dest_rep_qty_nan.to_csv('dest_rep_qty_nan.csv', index=False)"
      ],
      "metadata": {
        "id": "02ClDWx1zT7Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging destination report and master policy to identify gaps\n",
        "\n",
        "merged_df = pd.merge(mp, dest_rep, on='unique_key', how='left')"
      ],
      "metadata": {
        "id": "PCL_P-Wdhu_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#count the inventory target in master policy and the inventory target in the destination report to see the gap in creation\n",
        "\n",
        "inventory_target_sum = merged_df['inventory_target'].sum()\n",
        "inventory_target_dr_sum = merged_df['inventory_target_dr'].sum() if 'inventory_target_dr' in merged_df.columns else 0\n",
        "\n",
        "print(f\"Sum of inventory_target_mp: {inventory_target_sum}\")\n",
        "print(f\"Sum of inventory_target_dr: {inventory_target_dr_sum}\")"
      ],
      "metadata": {
        "id": "T-RvoOiUiH6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#putting a check to only see those stn'd where quantity was proposed in the master policy but nothing generated in destination report\n",
        "\n",
        "merged_df = merged_df[merged_df['inventory_target'] != merged_df['inventory_target_dr']]"
      ],
      "metadata": {
        "id": "Xb0yGRYay-9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new csv to store the rows where qty is present in master policy but missing in destination report.\n",
        "\n",
        "merged_df.to_csv('in_mp_not_in_dr.csv')"
      ],
      "metadata": {
        "id": "JHiAxtYXzso_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 3 : USING NETWORK LANE POLICY AND MASTER POLICY TO CHECK IF ANY PHASE MISMATCHES ARE THERE IN PHASE 3 & 5"
      ],
      "metadata": {
        "id": "6cx4jCcJ42M8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "RUN THIS FOR ONLY PHASE 3 & PHASE 5 REPORTS\n"
      ],
      "metadata": {
        "id": "Uv1xcSAK-07x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#read network lane policy after downloading from gcp\n",
        "\n",
        "net_lane_pol = pd.read_csv('network_lane_policy_path') #enter correct path"
      ],
      "metadata": {
        "id": "f1FWOJjb0Dfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#choose required columns only\n",
        "\n",
        "net_lane_pol = net_lane_pol[['Destination Cluster', 'Phase']]"
      ],
      "metadata": {
        "id": "rJu6JPsD10jp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#merging master policy with network lane policy to map phases from both files and finding mismatch\n",
        "\n",
        "merged_df_1 = pd.merge(mp, net_lane_pol, left_on='cluster', right_on='Destination Cluster', how='left')"
      ],
      "metadata": {
        "id": "0R69oQKs401j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keeping only phase 3 and 5 rows because only they might have a mismatch problem\n",
        "\n",
        "merged_df_1 = merged_df[merged_df_1['use_case'].isin(['nl_ds', 'nl_ds2'])]"
      ],
      "metadata": {
        "id": "jQ4M0xm27SUd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#create a new column called phase_master_policy , and pupulate the column using the use_case\n",
        "\n",
        "merged_df_1['phase_mp'] = np.where(merged_df_1['use_case'] == 'nl_ds', 3, 5)"
      ],
      "metadata": {
        "id": "Z8RIjnEI7t9m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep only those rows where there is mismatch in new dataframe called merged_df_mismatch\n",
        "\n",
        "merged_df_1_mismatch = merged_df_1[ ((merged_df_1['Phase'] == 3) & (merged_df_1['phase_mp'] != 3)) | ((merged_df_1['Phase'] == 5) & (merged_df_1['phase_mp'] != 5)) ]"
      ],
      "metadata": {
        "id": "AQmznqRc7--6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#keep unique incorrect phase matches\n",
        "\n",
        "merged_df_1_mismatch = merged_df_1_mismatch.drop_duplicates(subset=['cluster'])"
      ],
      "metadata": {
        "id": "hbNm9q3r8LRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#save to csv\n",
        "\n",
        "merged_df_1_mismatch.to_csv('mismatch_phase_nl_mp.csv')"
      ],
      "metadata": {
        "id": "iR4wO9l59Lf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ONCE ALL OF THIS IS DONE :"
      ],
      "metadata": {
        "id": "mdA63Lol9Vk4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "UPLOAD :\n",
        "1.   mismatch_phase_nl_mp.csv\n",
        "2.   in_mp_not_in_dr.csv\n",
        "3.   req_rep_error_quantified.csv\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iLAIQR7U9YNu"
      }
    }
  ]
}